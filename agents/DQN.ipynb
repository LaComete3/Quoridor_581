{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Input</b>:<br>\n",
    "\t$\\quad\\quad$ none<br>\n",
    "<b>Algorithm parameter</b>:<br>\n",
    "\t$\\quad\\quad$ discount factor $\\gamma$<br>\n",
    "\t$\\quad\\quad$ step size $\\alpha \\in (0,1]$<br>\n",
    "\t$\\quad\\quad$ small $\\epsilon > 0$<br><br>\n",
    "\n",
    "<b>FOR EACH</b> episode<br>\n",
    "\t$\\quad$ $\\mathbf{s} \\leftarrow \\text{env.reset}()$<br>\n",
    "\t$\\quad$ <b>DO</b> <br>\n",
    "\t\t$\\quad\\quad$ $\\mathbf{a} \\leftarrow \\epsilon\\text{-greedy}(\\mathbf{s}, \\hat{Q}_{\\mathbf{\\omega}}$ $)$<br>\n",
    "\t\t$\\quad\\quad$ $r, \\mathbf{s'} \\leftarrow \\text{env.step}(\\mathbf{a})$<br>\n",
    "\t\t$\\quad\\quad$ $y \\leftarrow\n",
    "\t\t\\begin{cases}\n",
    "\t\t\tr & \\text{for terminal } \\mathbf{s'}\\\\\n",
    "\t\t\tr + \\gamma \\max_{\\mathbf{a}^\\star \\in A} \\hat{Q}_{\\mathbf{\\omega}}(\\mathbf{s'})_{\\mathbf{a}^\\star} & \\text{for non-terminal } \\mathbf{s'}\n",
    "\t\t\\end{cases}$<br>\n",
    "\t\t$\\quad\\quad$ $\\mathbf{\\omega} \\leftarrow \\mathbf{\\omega} + \\alpha \\left[ y - \\hat{Q}_{\\mathbf{\\omega}}(\\mathbf{s})_{\\mathbf{a}} \\right] ~ \\nabla_{\\mathbf{\\omega}} \\hat{Q}_{\\mathbf{\\omega}}(\\mathbf{s})_{\\mathbf{a}}$ <br>\n",
    "\t\t$\\quad\\quad$ $\\mathbf{s} \\leftarrow \\mathbf{s'}$ <br>\n",
    "\t$\\quad$ <b>UNTIL</b> $\\mathbf{s}$ is final<br><br>\n",
    "<b>RETURN</b> $\\mathbf{\\omega}$ <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm # pour les barres de progression\n",
    "import random # pour epsilon greedy\n",
    "\n",
    "import collections # pour le relay buffer\n",
    "from typing import Callable, cast, List, Tuple, Union\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname('../qoridor'))))\n",
    "\n",
    "\n",
    "from qoridor.game import QoridorGame\n",
    "from qoridor.move import Move, MoveType\n",
    "from qoridor.board import WallOrientation\n",
    "from qoridor.visualization import QoridorVisualizer\n",
    "from agents.random_agent import RandomAgent\n",
    "from qoridor.environment import QoridorEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = QoridorGame(board_size=5, num_walls=3)\n",
    "visualizer = QoridorVisualizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game.state.get_observation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State to vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_to_vector(state):\n",
    "    \"\"\"Converts the state to a vector representation.\n",
    "\n",
    "    Args:\n",
    "        state (_type_): _description_\n",
    "    \"\"\"\n",
    "    player_1_pos = state.get_observation()['player_positions'][1] # tuple (x,y)\n",
    "    player_2_pos = state.get_observation()['player_positions'][2] # tuple (x,y)\n",
    "    walls_1 = state.get_observation()['walls_left'][1]  #int\n",
    "    walls_2 = state.get_observation()['walls_left'][2] #int\n",
    "    current_player = state.get_observation()['current_player'] # 1 ou 2 \n",
    "    horizontal_walls = state.get_observation()['horizontal_walls'].astype(int) # matrice (N-1)*(N-1) avec bit de présence\n",
    "    vertical_walls = state.get_observation()['vertical_walls'].astype(int) # matrice (N-1)*(N-1) avec bit de présence\n",
    "\n",
    "    state_vector = np.array([player_1_pos[0],player_1_pos[1],player_2_pos[0],player_2_pos[1],walls_1,walls_2,current_player])\n",
    "\n",
    "    horizontal_walls_vector = horizontal_walls.flatten()\n",
    "    vertical_walls_vector = vertical_walls.flatten()\n",
    "\n",
    "    full_state_vector = np.concatenate((state_vector,horizontal_walls_vector,vertical_walls_vector))\n",
    "\n",
    "    return full_state_vector\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_to_vector(game.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">  \n",
    "<b> Warning :</b> We could discuss the architecture of the network (fully connected, CNN etc)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A Q-Network implemented with PyTorch.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    layer1 : torch.nn.Linear\n",
    "        First fully connected layer.\n",
    "    layer2 : torch.nn.Linear\n",
    "        Second fully connected layer.\n",
    "    layer3 : torch.nn.Linear\n",
    "        Third fully connected layer.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    forward(x: torch.Tensor) -> torch.Tensor\n",
    "        Define the forward pass of the QNetwork.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_observations: int, n_actions: int, nn_l1: int, nn_l2: int):\n",
    "        \"\"\"\n",
    "        Initialize a new instance of QNetwork.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_observations : int\n",
    "            The size of the observation space.\n",
    "        n_actions : int\n",
    "            The size of the action space.\n",
    "        nn_l1 : int\n",
    "            The number of neurons on the first layer.\n",
    "        nn_l2 : int\n",
    "            The number of neurons on the second layer.\n",
    "        \"\"\"\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.layer1 = torch.nn.Linear(n_observations, nn_l1)\n",
    "        self.layer2 = torch.nn.Linear(nn_l1, nn_l2)\n",
    "        self.layer3 = torch.nn.Linear(nn_l2, n_actions)\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Define the forward pass of the QNetwork.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            The input tensor (state).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            The output tensor (Q-values).\n",
    "        \"\"\"\n",
    "        x = torch.relu(self.layer1(x))\n",
    "        x = torch.relu(self.layer2(x))\n",
    "        output_tensor = self.layer3(x) # linear activation = no activation function\n",
    "\n",
    "\n",
    "\n",
    "        return output_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epsilon Greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedy:\n",
    "    \"\"\"\n",
    "    An Epsilon-Greedy policy.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    epsilon : float\n",
    "        The initial probability of choosing a random action.\n",
    "    epsilon_min : float\n",
    "        The minimum probability of choosing a random action.\n",
    "    epsilon_decay : float\n",
    "        The decay rate for the epsilon value after each action.\n",
    "    env : ????\n",
    "    q_network : torch.nn.Module\n",
    "        The Q-Network used to estimate action values.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    __call__(state: np.ndarray) -> np.int64\n",
    "        Select an action for the given state using the epsilon-greedy policy.\n",
    "    decay_epsilon()\n",
    "        Decay the epsilon value after each action.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        epsilon_start: float,\n",
    "        epsilon_min: float,\n",
    "        epsilon_decay: float,\n",
    "        env: QoridorEnv,\n",
    "        q_network: torch.nn.Module,\n",
    "    ):\n",
    "       \n",
    "        self.epsilon = epsilon_start\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.env = env\n",
    "        self.q_network = q_network\n",
    "\n",
    "    def __call__(self, state: np.ndarray) -> np.int64:\n",
    "\n",
    "\n",
    "\n",
    "        if random.random() < self.epsilon:\n",
    "\n",
    "            action = self.env.get_legal_actions()\n",
    "            action = random.choice(action)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                action = self.env.get_legal_actions()\n",
    "                state_tensor = torch.tensor(state_to_vector(state), dtype=torch.float32).unsqueeze(0)\n",
    "                q_values = self.q_network(state_tensor)\n",
    "                q_values = [q_values[0][i].item() if i in action else -np.inf for i in range(len(q_values[0]))] \n",
    "                action = torch.argmax(torch.tensor(q_values)).item()\n",
    "\n",
    "        return action\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        \"\"\"\n",
    "        Decay the epsilon value after each episode.\n",
    "\n",
    "        The new epsilon value is the maximum of `epsilon_min` and the product of the current\n",
    "        epsilon value and `epsilon_decay`.\n",
    "        \"\"\"\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">  \n",
    "<b> Warning :</b> We need to check more the env.step and change the rewards (for the moment : +1 if victory -1 if loss)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"\n",
    "    A Replay Buffer.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    buffer : collections.deque\n",
    "        A double-ended queue where the transitions are stored.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    add(state: np.ndarray, action: np.int64, reward: float, next_state: np.ndarray, done: bool)\n",
    "        Add a new transition to the buffer.\n",
    "    sample(batch_size: int) -> Tuple[np.ndarray, float, float, np.ndarray, bool]\n",
    "        Sample a batch of transitions from the buffer.\n",
    "    __len__()\n",
    "        Return the current size of the buffer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, capacity: int):\n",
    "        \"\"\"\n",
    "        Initializes a ReplayBuffer instance.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        capacity : int\n",
    "            The maximum number of transitions that can be stored in the buffer.\n",
    "        \"\"\"\n",
    "        self.buffer: collections.deque = collections.deque(maxlen=capacity)\n",
    "\n",
    "    def add(\n",
    "        self,\n",
    "        state: np.ndarray,\n",
    "        action: np.int64,\n",
    "        reward: float,\n",
    "        next_state: np.ndarray,\n",
    "        done: bool,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Add a new transition to the buffer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        state : np.ndarray\n",
    "            The state vector of the added transition.\n",
    "        action : np.int64\n",
    "            The action of the added transition.\n",
    "        reward : float\n",
    "            The reward of the added transition.\n",
    "        next_state : np.ndarray\n",
    "            The next state vector of the added transition.\n",
    "        done : bool\n",
    "            The final state of the added transition.\n",
    "        \"\"\"\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(\n",
    "        self, batch_size: int\n",
    "    ) -> Tuple[np.ndarray, Tuple[int], Tuple[float], np.ndarray, Tuple[bool]]:\n",
    "        \"\"\"\n",
    "        Sample a batch of transitions from the buffer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_size : int\n",
    "            The number of transitions to sample.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[np.ndarray, float, float, np.ndarray, bool]\n",
    "            A batch of `batch_size` transitions.\n",
    "        \"\"\"\n",
    "        # Here, `random.sample(self.buffer, batch_size)`\n",
    "        # returns a list of tuples `(state, action, reward, next_state, done)`\n",
    "        # where:\n",
    "        # - `state`  and `next_state` are numpy arrays\n",
    "        # - `action` and `reward` are floats\n",
    "        # - `done` is a boolean\n",
    "        #\n",
    "        # `states, actions, rewards, next_states, dones = zip(*random.sample(self.buffer, batch_size))`\n",
    "        # generates 5 tuples `state`, `action`, `reward`, `next_state` and `done`, each having `batch_size` elements.\n",
    "        states, actions, rewards, next_states, dones = zip(*random.sample(self.buffer, batch_size))\n",
    "        return np.array(states), actions, rewards, np.array(next_states), dones\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the current size of the buffer.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            The current size of the buffer.\n",
    "        \"\"\"\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.random_agent import RandomAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train_dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn2_agent(\n",
    "    env: QoridorEnv, #! \n",
    "    q_network: torch.nn.Module,\n",
    "    target_q_network: torch.nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    loss_fn: Callable,\n",
    "    epsilon_greedy: EpsilonGreedy,\n",
    "    device: torch.device,\n",
    "    lr_scheduler: torch.optim.lr_scheduler.LRScheduler,\n",
    "    num_episodes: int,\n",
    "    gamma: float,\n",
    "    batch_size: int,\n",
    "    replay_buffer: ReplayBuffer,\n",
    "    target_q_network_sync_period: int,\n",
    ") -> List[float]:\n",
    "    \"\"\"\n",
    "    Train the Q-network on the given environment.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env : #! ???\n",
    "        The environment to train on.\n",
    "    q_network : torch.nn.Module\n",
    "        The Q-network to train.\n",
    "    target_q_network : torch.nn.Module\n",
    "        The target Q-network to use for estimating the target Q-values.\n",
    "    optimizer : torch.optim.Optimizer\n",
    "        The optimizer to use for training.\n",
    "    loss_fn : callable\n",
    "        The loss function to use for training.\n",
    "    epsilon_greedy : EpsilonGreedy\n",
    "        The epsilon-greedy policy to use for action selection.\n",
    "    device : torch.device\n",
    "        The device to use for PyTorch computations.\n",
    "    lr_scheduler : torch.optim.lr_scheduler.LRScheduler\n",
    "        The learning rate scheduler to adjust the learning rate during training.\n",
    "    num_episodes : int\n",
    "        The number of episodes to train for.\n",
    "    gamma : float\n",
    "        The discount factor for future rewards.\n",
    "    batch_size : int\n",
    "        The size of the batch to use for training.\n",
    "    replay_buffer : ReplayBuffer\n",
    "        The replay buffer storing the experiences with their priorities.\n",
    "    target_q_network_sync_period : int\n",
    "        The number of episodes after which the target Q-network should be updated with the weights of the Q-network.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[float]\n",
    "        A list of cumulated rewards per episode.\n",
    "    \"\"\"\n",
    "\n",
    "    opponent = RandomAgent()\n",
    "\n",
    "\n",
    "    iteration = 0\n",
    "    episode_reward_list = []\n",
    "\n",
    "    for episode_index in tqdm(range(1, num_episodes)):\n",
    "        env.reset()\n",
    "        state = env.game.state\n",
    "        state_vector = state_to_vector(state)\n",
    "        episode_reward = 0.0\n",
    "\n",
    "        for t in itertools.count():\n",
    "            # We suppose the agent dqn to be player 1\n",
    "            if env.game.state.get_observation()['current_player'] == 1:\n",
    "                action = epsilon_greedy(state) # epsilon greedy transform le game state en state vector\n",
    "\n",
    "                _ , reward, done, info = env.step(action) # step return (observation, reward, done, {steps : self.steps})\n",
    "                next_state = env.game.state\n",
    "                next_state_vector = state_to_vector(next_state)\n",
    "\n",
    "                replay_buffer.add(state_vector, action, float(reward), next_state_vector, done) # on lui donne le state vector et non le game state\n",
    "\n",
    "                episode_reward += float(reward)\n",
    "\n",
    "                # Update the q_network weights with a batch of experiences from the buffer\n",
    "\n",
    "                if len(replay_buffer) > batch_size:\n",
    "                    batch_states, batch_actions, batch_rewards, batch_next_states, batch_dones = replay_buffer.sample(batch_size)\n",
    "\n",
    "                    # Convert to PyTorch tensors\n",
    "                    batch_states_tensor = torch.tensor(np.array(batch_states, dtype=np.float32), dtype=torch.float32, device=device)\n",
    "                    batch_actions_tensor = torch.tensor(np.array(batch_actions, dtype=np.int64), dtype=torch.long, device=device)\n",
    "                    batch_rewards_tensor = torch.tensor(np.array(batch_rewards, dtype=np.float32), dtype=torch.float32, device=device)\n",
    "                    batch_next_states_tensor = torch.tensor(np.array(batch_next_states, dtype=np.float32), dtype=torch.float32, device=device)\n",
    "                    batch_dones_tensor = torch.tensor(np.array(batch_dones, dtype=bool), dtype=torch.bool, device=device)\n",
    "\n",
    "\n",
    "                    # Convertir le tenseur booléen en un tenseur d'entiers (bug sinon sur le 1-batch_dones_tensor)\n",
    "                    batch_dones_tensor = batch_dones_tensor.int()\n",
    "\n",
    "                                    # Vérifier les dimensions des tenseurs\n",
    "                    # print(\"batch_states_tensor shape:\", batch_states_tensor.shape)\n",
    "                    # print(\"batch_actions_tensor shape:\", batch_actions_tensor.shape)\n",
    "                    # print(\"batch_rewards_tensor shape:\", batch_rewards_tensor.shape)\n",
    "                    # print(\"batch_next_states_tensor shape:\", batch_next_states_tensor.shape)\n",
    "                    # print(\"batch_dones_tensor shape:\", batch_dones_tensor.shape)\n",
    "\n",
    "\n",
    "                    # Compute the target Q values for the batch\n",
    "                    with torch.no_grad():\n",
    "                        #! we had different implementation of the following in our labs.\n",
    "                        next_state_q_values = target_q_network(batch_next_states_tensor)\n",
    "                        best_action_index = torch.argmax(next_state_q_values, dim=1, keepdim=True) #! à contrôler\n",
    "                        targets = batch_rewards_tensor + (1 - batch_dones_tensor) * gamma * next_state_q_values.gather(1, best_action_index).squeeze(1)\n",
    "                    current_q_values = q_network(batch_states_tensor) #! à contrôler\n",
    "                    # print(\"current_q_values shape:\", current_q_values.shape)\n",
    "                    # print(\"targets shape:\", targets.shape)\n",
    "                    # print(\"batch_actions_tensor.unsqueeze(1) shape:\", batch_actions_tensor.unsqueeze(1).shape)\n",
    "                    \n",
    "                    loss = loss_fn(current_q_values.gather(1, batch_actions_tensor.unsqueeze(1)).squeeze(1), targets) #! à contrôler\n",
    "\n",
    "                    # Optimize the model\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    lr_scheduler.step()\n",
    "\n",
    "                # Update the target network\n",
    "                if iteration % target_q_network_sync_period == 0:\n",
    "                    target_q_network.load_state_dict(q_network.state_dict())\n",
    "                iteration += 1\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "                state = next_state\n",
    "            \n",
    "            else :\n",
    "                # It's the turn of the opponent\n",
    "                move = opponent.get_move(env.game)\n",
    "                action = env._move_to_action(move)\n",
    "                _ , reward, done, info = env.step(action)\n",
    "                state = env.game.state\n",
    "                state_vector = state_to_vector(state)\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "        episode_reward_list.append(episode_reward)\n",
    "        epsilon_greedy.decay_epsilon()\n",
    "\n",
    "    return episode_reward_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinimumExponentialLR(torch.optim.lr_scheduler.ExponentialLR):\n",
    "    def __init__(\n",
    "        self,\n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        lr_decay: float,\n",
    "        last_epoch: int = -1,\n",
    "        min_lr: float = 1e-6,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize a new instance of MinimumExponentialLR.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        optimizer : torch.optim.Optimizer\n",
    "            The optimizer whose learning rate should be scheduled.\n",
    "        lr_decay : float\n",
    "            The multiplicative factor of learning rate decay.\n",
    "        last_epoch : int, optional\n",
    "            The index of the last epoch. Default is -1.\n",
    "        min_lr : float, optional\n",
    "            The minimum learning rate. Default is 1e-6.\n",
    "        \"\"\"\n",
    "        self.min_lr = min_lr\n",
    "        super().__init__(optimizer, lr_decay, last_epoch=-1)\n",
    "\n",
    "    def get_lr(self) -> List[float]:\n",
    "        \"\"\"\n",
    "        Compute learning rate using chainable form of the scheduler.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        List[float]\n",
    "            The learning rates of each parameter group.\n",
    "        \"\"\"\n",
    "        return [\n",
    "            max(base_lr * self.gamma**self.last_epoch, self.min_lr)\n",
    "            for base_lr in self.base_lrs\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = QoridorEnv(5,3)\n",
    "\n",
    "NUMBER_OF_TRAININGS = 5  # Change the default (global) value here if you want a specific number of trainings for this exercise\n",
    "dqn2_trains_result_list: List[List[Union[int, float]]] = [[], [], []] \n",
    "\n",
    "for train_index in range(NUMBER_OF_TRAININGS):\n",
    "    # Instantiate required objects\n",
    "    observation_dim = 7 + (env.board_size-1)**2 * 2\n",
    "    num_actions = env.action_space_size\n",
    "\n",
    "    q_network = QNetwork(observation_dim, num_actions, nn_l1=256, nn_l2=128).to(device)\n",
    "    target_q_network = QNetwork(observation_dim, num_actions, nn_l1=256, nn_l2=128).to(device)\n",
    "    target_q_network.load_state_dict(q_network.state_dict())\n",
    "    optimizer = torch.optim.AdamW(q_network.parameters(), lr=0.004, amsgrad=True)\n",
    "    # lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.999)\n",
    "    lr_scheduler = MinimumExponentialLR(optimizer, lr_decay=0.97, min_lr=0.0001)\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "    epsilon_greedy = EpsilonGreedy(\n",
    "        epsilon_start=1,\n",
    "        epsilon_min=0.01,\n",
    "        epsilon_decay=0.996,\n",
    "        env=env,\n",
    "        q_network=q_network,\n",
    "    )\n",
    "\n",
    "    replay_buffer = ReplayBuffer(2000)\n",
    "\n",
    "    # Train the q-network\n",
    "\n",
    "    episode_reward_list = train_dqn2_agent(\n",
    "        env,\n",
    "        q_network,\n",
    "        target_q_network,\n",
    "        optimizer,\n",
    "        loss_fn,\n",
    "        epsilon_greedy,\n",
    "        device,\n",
    "        lr_scheduler,\n",
    "        num_episodes=150, #! to be changed\n",
    "        gamma=0.9,\n",
    "        batch_size=128,\n",
    "        replay_buffer=replay_buffer,\n",
    "        target_q_network_sync_period=30,\n",
    "    )\n",
    "    dqn2_trains_result_list[0].extend(range(len(episode_reward_list)))\n",
    "    dqn2_trains_result_list[1].extend(episode_reward_list)\n",
    "    dqn2_trains_result_list[2].extend([train_index for _ in episode_reward_list])\n",
    "\n",
    "dqn2_trains_result_df = pd.DataFrame(\n",
    "    np.array(dqn2_trains_result_list).T,\n",
    "    columns=[\"num_episodes\", \"mean_final_episode_reward\", \"training_index\"],\n",
    ")\n",
    "dqn2_trains_result_df[\"agent\"] = \"DQN 2013\"\n",
    "\n",
    "# Save the action-value estimation function\n",
    "\n",
    "torch.save(q_network, \"Test_dqn.pth\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.relplot(\n",
    "    x=\"num_episodes\",\n",
    "    y=\"mean_final_episode_reward\",\n",
    "    kind=\"line\",\n",
    "    hue=\"agent\",\n",
    "    estimator=None,\n",
    "    units=\"training_index\",\n",
    "    data=dqn2_trains_result_df,\n",
    "    height=7,\n",
    "    aspect=2,\n",
    "    alpha=0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state: np.ndarray, q_network: torch.nn.Module) -> int:\n",
    "    \"\"\"\n",
    "    Select the action with the highest Q-value for the given state.\n",
    "    \n",
    "    Args:\n",
    "        state: The current state as a numpy array.\n",
    "        q_network: The trained Q-network.\n",
    "        \n",
    "    Returns:\n",
    "        The action index with the highest Q-value.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "        q_values = q_network(state_tensor)\n",
    "        action = torch.argmax(q_values, dim=1).item()\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = QoridorEnv(5,3)\n",
    "visualizer = QoridorVisualizer()\n",
    "q_network = torch.load('Test_dqn.pth')\n",
    "\n",
    "while not game.is_game_over():\n",
    "    state = env.game.state\n",
    "    state_vector = state_to_vector(state)\n",
    "    action = select_action(state_vector, q_network)\n",
    "    env.step(action)\n",
    "    visualizer.render_game(env.game)\n",
    "    if game.is_game_over():\n",
    "        break\n",
    "\n",
    "    possible_moves = env.game.get_legal_moves()\n",
    "    for i, move in enumerate(possible_moves):\n",
    "        print(f\"{i}: {move}\")\n",
    "\n",
    "    move_index = int(input(\"Entrez l'index du mouvement à jouer : \"))\n",
    "    game.make_move(possible_moves[move_index])\n",
    "    visualizer.render_game(env.game)\n",
    "\n",
    "winner = env.game.get_winner()\n",
    "visualizer.render_game(env.game)\n",
    "print(f\"Winner : {winner}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CSC_52081_EP_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
