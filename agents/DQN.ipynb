{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Input</b>:<br>\n",
    "\t$\\quad\\quad$ none<br>\n",
    "<b>Algorithm parameter</b>:<br>\n",
    "\t$\\quad\\quad$ discount factor $\\gamma$<br>\n",
    "\t$\\quad\\quad$ step size $\\alpha \\in (0,1]$<br>\n",
    "\t$\\quad\\quad$ small $\\epsilon > 0$<br><br>\n",
    "\n",
    "<b>FOR EACH</b> episode<br>\n",
    "\t$\\quad$ $\\mathbf{s} \\leftarrow \\text{env.reset}()$<br>\n",
    "\t$\\quad$ <b>DO</b> <br>\n",
    "\t\t$\\quad\\quad$ $\\mathbf{a} \\leftarrow \\epsilon\\text{-greedy}(\\mathbf{s}, \\hat{Q}_{\\mathbf{\\omega}}$ $)$<br>\n",
    "\t\t$\\quad\\quad$ $r, \\mathbf{s'} \\leftarrow \\text{env.step}(\\mathbf{a})$<br>\n",
    "\t\t$\\quad\\quad$ $y \\leftarrow\n",
    "\t\t\\begin{cases}\n",
    "\t\t\tr & \\text{for terminal } \\mathbf{s'}\\\\\n",
    "\t\t\tr + \\gamma \\max_{\\mathbf{a}^\\star \\in A} \\hat{Q}_{\\mathbf{\\omega}}(\\mathbf{s'})_{\\mathbf{a}^\\star} & \\text{for non-terminal } \\mathbf{s'}\n",
    "\t\t\\end{cases}$<br>\n",
    "\t\t$\\quad\\quad$ $\\mathbf{\\omega} \\leftarrow \\mathbf{\\omega} + \\alpha \\left[ y - \\hat{Q}_{\\mathbf{\\omega}}(\\mathbf{s})_{\\mathbf{a}} \\right] ~ \\nabla_{\\mathbf{\\omega}} \\hat{Q}_{\\mathbf{\\omega}}(\\mathbf{s})_{\\mathbf{a}}$ <br>\n",
    "\t\t$\\quad\\quad$ $\\mathbf{s} \\leftarrow \\mathbf{s'}$ <br>\n",
    "\t$\\quad$ <b>UNTIL</b> $\\mathbf{s}$ is final<br><br>\n",
    "<b>RETURN</b> $\\mathbf{\\omega}$ <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm # pour les barres de progression\n",
    "import random # pour epsilon greedy\n",
    "\n",
    "import collections # pour le relay buffer\n",
    "from typing import Callable, cast, List, Tuple, Union\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname('../qoridor'))))\n",
    "\n",
    "\n",
    "from qoridor.game import QoridorGame\n",
    "from qoridor.move import Move, MoveType\n",
    "from qoridor.board import WallOrientation\n",
    "from qoridor.visualization import QoridorVisualizer\n",
    "from agents.random_agent import RandomAgent\n",
    "from qoridor.environment import QoridorEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = QoridorGame(board_size=5, num_walls=3)\n",
    "visualizer = QoridorVisualizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'board_size': 5,\n",
       " 'player_positions': {1: (0, 2), 2: (4, 2)},\n",
       " 'walls_left': {1: 3, 2: 3},\n",
       " 'horizontal_walls': array([[False, False, False, False],\n",
       "        [False, False, False, False],\n",
       "        [False, False, False, False],\n",
       "        [False, False, False, False]]),\n",
       " 'vertical_walls': array([[False, False, False, False],\n",
       "        [False, False, False, False],\n",
       "        [False, False, False, False],\n",
       "        [False, False, False, False]]),\n",
       " 'current_player': 1,\n",
       " 'done': False,\n",
       " 'winner': None}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.state.get_observation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State to vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_to_vector(state):\n",
    "    \"\"\"Converts the state to a vector representation.\n",
    "\n",
    "    Args:\n",
    "        state (_type_): _description_\n",
    "    \"\"\"\n",
    "    player_1_pos = state.get_observation()['player_positions'][1] # tuple (x,y)\n",
    "    player_2_pos = state.get_observation()['player_positions'][2] # tuple (x,y)\n",
    "    walls_1 = state.get_observation()['walls_left'][1]  #int\n",
    "    walls_2 = state.get_observation()['walls_left'][2] #int\n",
    "    current_player = state.get_observation()['current_player'] # 1 ou 2 \n",
    "    horizontal_walls = state.get_observation()['horizontal_walls'].astype(int) # matrice (N-1)*(N-1) avec bit de présence\n",
    "    vertical_walls = state.get_observation()['vertical_walls'].astype(int) # matrice (N-1)*(N-1) avec bit de présence\n",
    "\n",
    "    state_vector = np.array([player_1_pos[0],player_1_pos[1],player_2_pos[0],player_2_pos[1],walls_1,walls_2,current_player])\n",
    "\n",
    "    horizontal_walls_vector = horizontal_walls.flatten()\n",
    "    vertical_walls_vector = vertical_walls.flatten()\n",
    "\n",
    "    full_state_vector = np.concatenate((state_vector,horizontal_walls_vector,vertical_walls_vector))\n",
    "\n",
    "    return full_state_vector\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_to_vector(state):\n",
    "    \"\"\"Converts the state to a vector representation.\n",
    "\n",
    "    Args:\n",
    "        state (_type_): _description_\n",
    "    \"\"\"\n",
    "    player_1_pos = state.get_observation()['player_positions'][1] # tuple (x,y)\n",
    "    player_2_pos = state.get_observation()['player_positions'][2] # tuple (x,y)\n",
    "    walls_1 = state.get_observation()['walls_left'][1]  #int\n",
    "    walls_2 = state.get_observation()['walls_left'][2] #int\n",
    "    current_player = state.get_observation()['current_player'] # 1 ou 2 \n",
    "    horizontal_walls = state.get_observation()['horizontal_walls'].astype(int) # matrice (N-1)*(N-1) avec bit de présence\n",
    "    vertical_walls = state.get_observation()['vertical_walls'].astype(int) # matrice (N-1)*(N-1) avec bit de présence\n",
    "\n",
    "    state_vector = np.array([player_1_pos[0],player_1_pos[1],player_2_pos[0],player_2_pos[1],walls_1,walls_2,current_player])\n",
    "\n",
    "    horizontal_walls_vector = horizontal_walls.flatten()\n",
    "    vertical_walls_vector = vertical_walls.flatten()\n",
    "\n",
    "    full_state_vector = np.concatenate((state_vector,horizontal_walls_vector,vertical_walls_vector))\n",
    "\n",
    "    return full_state_vector\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 4, 2, 3, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_to_vector(game.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">  \n",
    "<b> Warning :</b> We could discuss the architecture of the network (fully connected, CNN etc)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A Q-Network implemented with PyTorch.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    layer1 : torch.nn.Linear\n",
    "        First fully connected layer.\n",
    "    layer2 : torch.nn.Linear\n",
    "        Second fully connected layer.\n",
    "    layer3 : torch.nn.Linear\n",
    "        Third fully connected layer.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    forward(x: torch.Tensor) -> torch.Tensor\n",
    "        Define the forward pass of the QNetwork.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_observations: int, n_actions: int, nn_l1: int, nn_l2: int):\n",
    "        \"\"\"\n",
    "        Initialize a new instance of QNetwork.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_observations : int\n",
    "            The size of the observation space.\n",
    "        n_actions : int\n",
    "            The size of the action space.\n",
    "        nn_l1 : int\n",
    "            The number of neurons on the first layer.\n",
    "        nn_l2 : int\n",
    "            The number of neurons on the second layer.\n",
    "        \"\"\"\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.layer1 = torch.nn.Linear(n_observations, nn_l1)\n",
    "        self.layer2 = torch.nn.Linear(nn_l1, nn_l2)\n",
    "        self.layer3 = torch.nn.Linear(nn_l2, n_actions)\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Define the forward pass of the QNetwork.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            The input tensor (state).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            The output tensor (Q-values).\n",
    "        \"\"\"\n",
    "        x = torch.relu(self.layer1(x))\n",
    "        x = torch.relu(self.layer2(x))\n",
    "        output_tensor = self.layer3(x) # linear activation = no activation function\n",
    "\n",
    "\n",
    "\n",
    "        return output_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epsilon Greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedy:\n",
    "    \"\"\"\n",
    "    An Epsilon-Greedy policy.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    epsilon : float\n",
    "        The initial probability of choosing a random action.\n",
    "    epsilon_min : float\n",
    "        The minimum probability of choosing a random action.\n",
    "    epsilon_decay : float\n",
    "        The decay rate for the epsilon value after each action.\n",
    "    env : ????\n",
    "    q_network : torch.nn.Module\n",
    "        The Q-Network used to estimate action values.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    __call__(state: np.ndarray) -> np.int64\n",
    "        Select an action for the given state using the epsilon-greedy policy.\n",
    "    decay_epsilon()\n",
    "        Decay the epsilon value after each action.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        epsilon_start: float,\n",
    "        epsilon_min: float,\n",
    "        epsilon_decay: float,\n",
    "        env: QoridorEnv,\n",
    "        q_network: torch.nn.Module,\n",
    "    ):\n",
    "       \n",
    "        self.epsilon = epsilon_start\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.env = env\n",
    "        self.q_network = q_network\n",
    "\n",
    "    def __call__(self, state: np.ndarray) -> np.int64:\n",
    "        #! we have to filter the actions that are not legal\n",
    "\n",
    "\n",
    "\n",
    "        if random.random() < self.epsilon:\n",
    "\n",
    "            action = self.env.get_legal_actions()\n",
    "            action = random.choice(action)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                action = self.env.get_legal_actions()\n",
    "                state_tensor = torch.tensor(state_to_vector(state), dtype=torch.float32).unsqueeze(0)\n",
    "                q_values = self.q_network(state_tensor)\n",
    "                q_values = [q_values[0][i].item() if i in action else -np.inf for i in range(len(q_values[0]))] #! GPT à vérifier\n",
    "                action = torch.argmax(torch.tensor(q_values)).item()\n",
    "\n",
    "        return action\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        \"\"\"\n",
    "        Decay the epsilon value after each episode.\n",
    "\n",
    "        The new epsilon value is the maximum of `epsilon_min` and the product of the current\n",
    "        epsilon value and `epsilon_decay`.\n",
    "        \"\"\"\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">  \n",
    "<b> Warning :</b> We need to check more the env.step and change the rewards (for the moment : +1 if victory -1 if loss)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"\n",
    "    A Replay Buffer.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    buffer : collections.deque\n",
    "        A double-ended queue where the transitions are stored.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    add(state: np.ndarray, action: np.int64, reward: float, next_state: np.ndarray, done: bool)\n",
    "        Add a new transition to the buffer.\n",
    "    sample(batch_size: int) -> Tuple[np.ndarray, float, float, np.ndarray, bool]\n",
    "        Sample a batch of transitions from the buffer.\n",
    "    __len__()\n",
    "        Return the current size of the buffer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, capacity: int):\n",
    "        \"\"\"\n",
    "        Initializes a ReplayBuffer instance.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        capacity : int\n",
    "            The maximum number of transitions that can be stored in the buffer.\n",
    "        \"\"\"\n",
    "        self.buffer: collections.deque = collections.deque(maxlen=capacity)\n",
    "\n",
    "    def add(\n",
    "        self,\n",
    "        state: np.ndarray,\n",
    "        action: np.int64,\n",
    "        reward: float,\n",
    "        next_state: np.ndarray,\n",
    "        done: bool,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Add a new transition to the buffer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        state : np.ndarray\n",
    "            The state vector of the added transition.\n",
    "        action : np.int64\n",
    "            The action of the added transition.\n",
    "        reward : float\n",
    "            The reward of the added transition.\n",
    "        next_state : np.ndarray\n",
    "            The next state vector of the added transition.\n",
    "        done : bool\n",
    "            The final state of the added transition.\n",
    "        \"\"\"\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(\n",
    "        self, batch_size: int\n",
    "    ) -> Tuple[np.ndarray, Tuple[int], Tuple[float], np.ndarray, Tuple[bool]]:\n",
    "        \"\"\"\n",
    "        Sample a batch of transitions from the buffer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_size : int\n",
    "            The number of transitions to sample.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[np.ndarray, float, float, np.ndarray, bool]\n",
    "            A batch of `batch_size` transitions.\n",
    "        \"\"\"\n",
    "        # Here, `random.sample(self.buffer, batch_size)`\n",
    "        # returns a list of tuples `(state, action, reward, next_state, done)`\n",
    "        # where:\n",
    "        # - `state`  and `next_state` are numpy arrays\n",
    "        # - `action` and `reward` are floats\n",
    "        # - `done` is a boolean\n",
    "        #\n",
    "        # `states, actions, rewards, next_states, dones = zip(*random.sample(self.buffer, batch_size))`\n",
    "        # generates 5 tuples `state`, `action`, `reward`, `next_state` and `done`, each having `batch_size` elements.\n",
    "        states, actions, rewards, next_states, dones = zip(*random.sample(self.buffer, batch_size))\n",
    "        return np.array(states), actions, rewards, np.array(next_states), dones\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the current size of the buffer.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            The current size of the buffer.\n",
    "        \"\"\"\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train_dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn1_agent(\n",
    "    env: QoridorEnv, #! \n",
    "    q_network: torch.nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    loss_fn: Callable,\n",
    "    epsilon_greedy: EpsilonGreedy,\n",
    "    device: torch.device,\n",
    "    lr_scheduler: torch.optim.lr_scheduler.LRScheduler,\n",
    "    num_episodes: int,\n",
    "    gamma: float,\n",
    "    batch_size: int,\n",
    "    replay_buffer: ReplayBuffer,\n",
    ") -> List[float]:\n",
    "    \"\"\"\n",
    "    Train the Q-network on the given environment.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env : #! ???\n",
    "        The environment to train on.\n",
    "    q_network : torch.nn.Module\n",
    "        The Q-network to train.\n",
    "    optimizer : torch.optim.Optimizer\n",
    "        The optimizer to use for training.\n",
    "    loss_fn : callable\n",
    "        The loss function to use for training.\n",
    "    epsilon_greedy : EpsilonGreedy\n",
    "        The epsilon-greedy policy to use for action selection.\n",
    "    device : torch.device\n",
    "        The device to use for PyTorch computations.\n",
    "    lr_scheduler : torch.optim.lr_scheduler.LRScheduler\n",
    "        The learning rate scheduler to adjust the learning rate during training.\n",
    "    num_episodes : int\n",
    "        The number of episodes to train for.\n",
    "    gamma : float\n",
    "        The discount factor for future rewards.\n",
    "    batch_size : int\n",
    "        The size of the batch to use for training.\n",
    "    replay_buffer : ReplayBuffer\n",
    "        The replay buffer storing the experiences with their priorities.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[float]\n",
    "        A list of cumulated rewards per episode.\n",
    "    \"\"\"\n",
    "    episode_reward_list = []\n",
    "\n",
    "    for episode_index in tqdm(range(1, num_episodes)):\n",
    "        env.reset()\n",
    "        state = env.game.state\n",
    "        episode_reward = 0.0\n",
    "\n",
    "        for t in itertools.count():\n",
    "            # Get action, next_state and reward\n",
    "\n",
    "            action = epsilon_greedy(state)\n",
    "\n",
    "            _ , reward, terminated, info = env.step(action) # step return (observation, reward, done, {steps : self.steps})\n",
    "            next_state = env.game.state\n",
    "            truncated = (info['steps'] >= 1000)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            replay_buffer.add(state, action, float(reward), next_state, done)\n",
    "\n",
    "            episode_reward += float(reward)\n",
    "\n",
    "            # Update the q_network weights with a batch of experiences from the buffer\n",
    "\n",
    "            if len(replay_buffer) > batch_size:\n",
    "                batch_states, batch_actions, batch_rewards, batch_next_states, batch_dones = replay_buffer.sample(batch_size)\n",
    "\n",
    "                # Convert to PyTorch tensors\n",
    "                batch_states_tensor = torch.tensor(batch_states, dtype=torch.float32, device=device)\n",
    "                batch_actions_tensor = torch.tensor(batch_actions, dtype=torch.long, device=device)\n",
    "                batch_rewards_tensor = torch.tensor(batch_rewards, dtype=torch.float32, device=device)\n",
    "                batch_next_states_tensor = torch.tensor(batch_next_states, dtype=torch.float32, device=device)\n",
    "                batch_dones_tensor = torch.tensor(batch_dones, dtype=torch.float32, device=device)\n",
    "\n",
    "                # Compute the target Q values for the batch\n",
    "                with torch.no_grad():\n",
    "                    #! we had different implementation of the following in our labs.\n",
    "                    next_state_q_values = q_network(batch_next_states_tensor)\n",
    "                    best_action_index = torch.argmax(next_state_q_values, dim=1) #! à contrôler\n",
    "                    targets = batch_rewards_tensor + (1 - batch_dones_tensor) * gamma * next_state_q_values.gather(1, best_action_index.unsqueeze(1)).squeeze(1) #! à contrôler\n",
    "                current_q_values = q_network(batch_states_tensor) #! à contrôler\n",
    "                loss = loss_fn(current_q_values.gather(1, batch_actions_tensor.unsqueeze(1)).squeeze(1), targets) #! à contrôler\n",
    "\n",
    "                # Optimize the model\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                lr_scheduler.step()\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        episode_reward_list.append(episode_reward)\n",
    "        epsilon_greedy.decay_epsilon()\n",
    "\n",
    "    return episode_reward_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinimumExponentialLR(torch.optim.lr_scheduler.ExponentialLR):\n",
    "    def __init__(\n",
    "        self,\n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        lr_decay: float,\n",
    "        last_epoch: int = -1,\n",
    "        min_lr: float = 1e-6,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize a new instance of MinimumExponentialLR.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        optimizer : torch.optim.Optimizer\n",
    "            The optimizer whose learning rate should be scheduled.\n",
    "        lr_decay : float\n",
    "            The multiplicative factor of learning rate decay.\n",
    "        last_epoch : int, optional\n",
    "            The index of the last epoch. Default is -1.\n",
    "        min_lr : float, optional\n",
    "            The minimum learning rate. Default is 1e-6.\n",
    "        \"\"\"\n",
    "        self.min_lr = min_lr\n",
    "        super().__init__(optimizer, lr_decay, last_epoch=-1)\n",
    "\n",
    "    def get_lr(self) -> List[float]:\n",
    "        \"\"\"\n",
    "        Compute learning rate using chainable form of the scheduler.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        List[float]\n",
    "            The learning rates of each parameter group.\n",
    "        \"\"\"\n",
    "        return [\n",
    "            max(base_lr * self.gamma**self.last_epoch, self.min_lr)\n",
    "            for base_lr in self.base_lrs\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b33f7331dfd4b68980cccf95c725d42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/149 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid horizontal wall position at column 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 29\u001b[0m\n\u001b[1;32m     25\u001b[0m replay_buffer \u001b[38;5;241m=\u001b[39m ReplayBuffer(\u001b[38;5;241m2000\u001b[39m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Train the q-network\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m episode_reward_list \u001b[38;5;241m=\u001b[39m train_dqn1_agent(\n\u001b[1;32m     30\u001b[0m     env,\n\u001b[1;32m     31\u001b[0m     q_network,\n\u001b[1;32m     32\u001b[0m     optimizer,\n\u001b[1;32m     33\u001b[0m     loss_fn,\n\u001b[1;32m     34\u001b[0m     epsilon_greedy,\n\u001b[1;32m     35\u001b[0m     device,\n\u001b[1;32m     36\u001b[0m     lr_scheduler,\n\u001b[1;32m     37\u001b[0m     num_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m150\u001b[39m,\n\u001b[1;32m     38\u001b[0m     gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m,\n\u001b[1;32m     39\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m,\n\u001b[1;32m     40\u001b[0m     replay_buffer\u001b[38;5;241m=\u001b[39mreplay_buffer,\n\u001b[1;32m     41\u001b[0m )\n\u001b[1;32m     42\u001b[0m dqn1_trains_result_list[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(episode_reward_list)))\n\u001b[1;32m     43\u001b[0m dqn1_trains_result_list[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mextend(episode_reward_list)\n",
      "Cell \u001b[0;32mIn[64], line 57\u001b[0m, in \u001b[0;36mtrain_dqn1_agent\u001b[0;34m(env, q_network, optimizer, loss_fn, epsilon_greedy, device, lr_scheduler, num_episodes, gamma, batch_size, replay_buffer)\u001b[0m\n\u001b[1;32m     52\u001b[0m episode_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mcount():\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;66;03m# Get action, next_state and reward\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     action \u001b[38;5;241m=\u001b[39m epsilon_greedy(state)\n\u001b[1;32m     59\u001b[0m     _ , reward, terminated, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action) \u001b[38;5;66;03m# step return (observation, reward, done, {steps : self.steps})\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     next_state \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mgame\u001b[38;5;241m.\u001b[39mstate\n",
      "Cell \u001b[0;32mIn[62], line 47\u001b[0m, in \u001b[0;36mEpsilonGreedy.__call__\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, state: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mint64:\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m#! we have to filter the actions that are not legal\u001b[39;00m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m random\u001b[38;5;241m.\u001b[39mrandom() \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon:\n\u001b[0;32m---> 47\u001b[0m         action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mget_legal_actions()\n\u001b[1;32m     48\u001b[0m         action \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mchoice(action)\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/3A/CSC_52081_EP/Projet/Quoridor_581/qoridor/environment.py:274\u001b[0m, in \u001b[0;36mQoridorEnv.get_legal_actions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;124;03mGet all legal actions from the current state.\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;124;03mReturns:\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;124;03m    List of legal action indices\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    273\u001b[0m legal_moves \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgame\u001b[38;5;241m.\u001b[39mget_legal_moves()\n\u001b[0;32m--> 274\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_move_to_action(move) \u001b[38;5;28;01mfor\u001b[39;00m move \u001b[38;5;129;01min\u001b[39;00m legal_moves]\n",
      "File \u001b[0;32m~/Desktop/3A/CSC_52081_EP/Projet/Quoridor_581/qoridor/environment.py:164\u001b[0m, in \u001b[0;36mQoridorEnv._move_to_action\u001b[0;34m(self, move)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_horizontal:\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# Cannot place horizontal walls at the rightmost column\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m col \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mboard_size \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m--> 164\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid horizontal wall position at column \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    165\u001b[0m     base \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_move_actions\n\u001b[1;32m    166\u001b[0m     index \u001b[38;5;241m=\u001b[39m row \u001b[38;5;241m*\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mboard_size \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m+\u001b[39m col\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid horizontal wall position at column 3"
     ]
    }
   ],
   "source": [
    "env = QoridorEnv(5,3)\n",
    "\n",
    "NUMBER_OF_TRAININGS = 1  # Change the default (global) value here if you want a specific number of trainings for this exercise\n",
    "dqn1_trains_result_list: List[List[Union[int, float]]] = [[], [], []] \n",
    "\n",
    "for train_index in range(NUMBER_OF_TRAININGS):\n",
    "    # Instantiate required objects\n",
    "    observation_dim = 7 + (env.board_size-1)**2 * 2\n",
    "    num_actions = env.action_space_size\n",
    "\n",
    "    q_network = QNetwork(observation_dim, num_actions, nn_l1=256, nn_l2=128).to(device)\n",
    "    optimizer = torch.optim.AdamW(q_network.parameters(), lr=0.004, amsgrad=True)\n",
    "    # lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.999)\n",
    "    lr_scheduler = MinimumExponentialLR(optimizer, lr_decay=0.97, min_lr=0.0001)\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "    epsilon_greedy = EpsilonGreedy(\n",
    "        epsilon_start=0.82,\n",
    "        epsilon_min=0.013,\n",
    "        epsilon_decay=0.9675,\n",
    "        env=env,\n",
    "        q_network=q_network,\n",
    "    )\n",
    "\n",
    "    replay_buffer = ReplayBuffer(2000)\n",
    "\n",
    "    # Train the q-network\n",
    "\n",
    "    episode_reward_list = train_dqn1_agent(\n",
    "        env,\n",
    "        q_network,\n",
    "        optimizer,\n",
    "        loss_fn,\n",
    "        epsilon_greedy,\n",
    "        device,\n",
    "        lr_scheduler,\n",
    "        num_episodes=150,\n",
    "        gamma=0.9,\n",
    "        batch_size=128,\n",
    "        replay_buffer=replay_buffer,\n",
    "    )\n",
    "    dqn1_trains_result_list[0].extend(range(len(episode_reward_list)))\n",
    "    dqn1_trains_result_list[1].extend(episode_reward_list)\n",
    "    dqn1_trains_result_list[2].extend([train_index for _ in episode_reward_list])\n",
    "\n",
    "dqn1_trains_result_df = pd.DataFrame(\n",
    "    np.array(dqn1_trains_result_list).T,\n",
    "    columns=[\"num_episodes\", \"mean_final_episode_reward\", \"training_index\"],\n",
    ")\n",
    "dqn1_trains_result_df[\"agent\"] = \"DQN 2013\"\n",
    "\n",
    "# Save the action-value estimation function\n",
    "\n",
    "# torch.save(q_network, \"\" / \"Test_Q.pth\")\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CSC_52081_EP_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
