{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "# Ajouter le chemin du répertoire contenant le module 'qoridor'\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "from qoridor.game import QoridorGame, Move, MoveType, WallOrientation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to use the Gym convention for our environment\n",
    "\n",
    "class QoridorEnv :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuoridorEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "    \n",
    "    def __init__(self, board_size=5, num_walls=3):\n",
    "        super(QuoridorEnv, self).__init__()\n",
    "        self.board_size = board_size\n",
    "        self.num_walls = num_walls\n",
    "        self.game = QoridorGame(board_size, num_walls)\n",
    "        \n",
    "        # Define action and observation space\n",
    "        # Actions: move (row, col) or place wall (row, col, orientation)\n",
    "        self.action_space = spaces.Discrete(board_size * board_size + 2 * (board_size - 1) * (board_size - 1))\n",
    "        \n",
    "        # Observation: board state, player positions, remaining walls\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(board_size, board_size, 3), dtype=np.float32)\n",
    "        \n",
    "    def reset(self):\n",
    "        self.game = QoridorGame(self.board_size, self.num_walls)\n",
    "        return self._get_observation()\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Decode action\n",
    "        if action < self.board_size * self.board_size:\n",
    "            row, col = divmod(action, self.board_size)\n",
    "            move = Move(MoveType.MOVE, (row, col))\n",
    "        else:\n",
    "            action -= self.board_size * self.board_size\n",
    "            if action < (self.board_size - 1) * (self.board_size - 1):\n",
    "                row, col = divmod(action, self.board_size - 1)\n",
    "                move = Move(MoveType.WALL, (row, col), WallOrientation.HORIZONTAL)\n",
    "            else:\n",
    "                action -= (self.board_size - 1) * (self.board_size - 1)\n",
    "                row, col = divmod(action, self.board_size - 1)\n",
    "                move = Move(MoveType.WALL, (row, col), WallOrientation.VERTICAL)\n",
    "        \n",
    "        # Apply action\n",
    "        reward = 0\n",
    "        done = False\n",
    "        if self.game.make_move(move):\n",
    "            reward = 1\n",
    "            if self.game.is_game_over():\n",
    "                done = True\n",
    "                reward = 10\n",
    "        else:\n",
    "            reward = -1\n",
    "        \n",
    "        return self._get_observation(), reward, done, {}\n",
    "    \n",
    "    def render(self, mode='human'):\n",
    "        # Optionally implement rendering logic\n",
    "        self.game.render()\n",
    "    \n",
    "    def close(self):\n",
    "        # Optionally implement cleanup logic\n",
    "        pass\n",
    "    \n",
    "    def _get_observation(self):\n",
    "        board = np.zeros((self.board_size, self.board_size, 3), dtype=np.float32)\n",
    "        for row in range(self.board_size):\n",
    "            for col in range(self.board_size):\n",
    "                if self.game.state.board.horizontal_walls[row, col]:\n",
    "                    board[row, col, 0] = 1\n",
    "                if self.game.state.board.vertical_walls[row, col]:\n",
    "                    board[row, col, 1] = 1\n",
    "        board[self.game.state.player1_pos[0], self.game.state.player1_pos[1], 2] = 1\n",
    "        board[self.game.state.player2_pos[0], self.game.state.player2_pos[1], 2] = 1\n",
    "        return board\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_shape[0], 32, kernel_size=3, stride=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1)\n",
    "        self.fc1 = nn.Linear(self._get_conv_output(input_shape), 128)\n",
    "        self.fc2 = nn.Linear(128, num_actions)\n",
    "    \n",
    "    def _get_conv_output(self, shape):\n",
    "        o = torch.zeros(1, *shape)\n",
    "        o = self.conv1(o)\n",
    "        o = self.conv2(o)\n",
    "        return int(np.prod(o.size()))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_shape, num_actions):\n",
    "        self.state_shape = state_shape\n",
    "        self.num_actions = num_actions\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95  # discount rate\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = DQN(state_shape, num_actions).to(device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        self.criterion = nn.MSELoss()\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.num_actions)\n",
    "        state = torch.FloatTensor(state).to(device)\n",
    "        with torch.no_grad():\n",
    "            act_values = self.model(state)\n",
    "        return np.argmax(act_values.cpu().data.numpy())\n",
    "    \n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            state = torch.FloatTensor(state).to(device)\n",
    "            next_state = torch.FloatTensor(next_state).to(device)\n",
    "            reward = torch.FloatTensor([reward]).to(device)\n",
    "            done = torch.FloatTensor([done]).to(device)\n",
    "            \n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = reward + self.gamma * torch.max(self.model(next_state))\n",
    "            target_f = self.model(state)\n",
    "            target_f = target_f.clone()\n",
    "            target_f[action] = target\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss = self.criterion(self.model(state), target_f)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 4 is out of bounds for axis 1 with size 4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(episodes):\n\u001b[0;32m---> 11\u001b[0m     state \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m     12\u001b[0m     state \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mtranspose(state, (\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))  \u001b[38;5;66;03m# Convert to channels first\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     state \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(state, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Add batch dimension\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[19], line 19\u001b[0m, in \u001b[0;36mQuoridorEnv.reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreset\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgame \u001b[38;5;241m=\u001b[39m QoridorGame(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mboard_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_walls)\n\u001b[0;32m---> 19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_observation()\n",
      "Cell \u001b[0;32mIn[19], line 61\u001b[0m, in \u001b[0;36mQuoridorEnv._get_observation\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mboard_size):\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mboard_size):\n\u001b[0;32m---> 61\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgame\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mboard\u001b[38;5;241m.\u001b[39mhorizontal_walls[row, col]:\n\u001b[1;32m     62\u001b[0m             board[row, col, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     63\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgame\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mboard\u001b[38;5;241m.\u001b[39mvertical_walls[row, col]:\n",
      "\u001b[0;31mIndexError\u001b[0m: index 4 is out of bounds for axis 1 with size 4"
     ]
    }
   ],
   "source": [
    "# Entraînement du modèle DQN\n",
    "env = QuoridorEnv()\n",
    "state_shape = (3, env.board_size, env.board_size)  # Channels first for PyTorch\n",
    "num_actions = env.action_space.n\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "agent = DQNAgent(state_shape, num_actions)\n",
    "episodes = 100\n",
    "batch_size = 32\n",
    "\n",
    "for e in range(episodes):\n",
    "    state = env.reset()\n",
    "    state = np.transpose(state, (2, 0, 1))  # Convert to channels first\n",
    "    state = np.expand_dims(state, axis=0)  # Add batch dimension\n",
    "    for time in range(500):\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state = np.transpose(next_state, (2, 0, 1))  # Convert to channels first\n",
    "        next_state = np.expand_dims(next_state, axis=0)  # Add batch dimension\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            print(f\"Episode: {e}/{episodes}, score: {time}, e: {agent.epsilon:.2}\")\n",
    "            break\n",
    "        if len(agent.memory) > batch_size:\n",
    "            agent.replay(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CSC_52081_EP_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
